# -*- coding: utf-8 -*-
"""Vodafone_Churn_Model.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1at-Ihkw0na9lxmk18QuQgPYnvFudatE1
"""

"""
Customer Churn Analysis and Prediction Script

This script take anonymised data from Vodafone and builds a churn prediction model on customers from the last month.
It then uses the information from the churn model to generate a personalised retention email.

Code Structure

Part 1 - Churn Model
 1. Setup
 2. Exploration
 3. Pre-Processing
 4. Logistic Regression Model as a Baseline
    a) Model Training
    b) Model Evaluation
    c) Model Interpretation
 5. XGBoost Model to Improve Accuracy
    a) Model Training
    b) Model Evaluation
    c) Model Interpretation

Part 2 - Generating a Retention Email
 1. Prepare Prompt Input
 2. Generate Prompt
 3. Generate Email

"""

"""
Part 1 - Churn Model
"""

"""
1. Setup
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    precision_recall_curve,
    average_precision_score,
    roc_curve,
    auc
)

import xgboost as xgb
import shap

import google.generativeai as genai

# Set up Google API key if using Gemini
os.environ["GOOGLE_API_KEY"] = "AIzaSyBHrLJ3lXbTFy2x5ioP9gk2SyGlO1L3Wvc"

# Load dataset
file_path = '/Vodafone_Customer_Churn_Sample_Dataset.csv'
df = pd.read_csv(file_path)

"""
2. Exploration
"""

# Basic info and sanity checks
print("DataFrame Information:")
df.info()

print("\nMissing Values Percentage:")
print(df.isnull().mean() * 100)

print("\nSummary Statistics for Numeric Variables:")
print(df.describe())

print("\nChurn Variable Analysis:")
print(df['Churn'].value_counts())
print(df['Churn'].value_counts(normalize=True) * 100)

# Convert TotalCharges to numeric, imputing invalid values as 0
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)
print("\nTotalCharges column converted to:", df['TotalCharges'].dtype)

# Visualisation: Separate features from target
target_variable = 'Churn'
feature_variables = df.columns.drop([target_variable, 'customerID'])

# Plot categorical feature distributions by churn
for column in feature_variables:
    if df[column].dtype == 'object' or df[column].nunique() < 10:
        plt.figure(figsize=(10, 6))
        sns.countplot(data=df, x=column, hue=target_variable)
        plt.title(f'Distribution of {column} by {target_variable}')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

# Plot numeric feature distributions by churn
for column in feature_variables:
    if df[column].dtype != 'object' and df[column].nunique() >= 10:
        plt.figure(figsize=(10, 6))
        sns.histplot(data=df, x=column, hue=target_variable, multiple="stack", kde=True)
        plt.title(f'Distribution of {column} by {target_variable}')
        plt.tight_layout()
        plt.show()

# Plot stacked bar plots for categorical churn proportions
for column in feature_variables:
    if df[column].dtype == 'object' or df[column].nunique() < 10:
        ctab = pd.crosstab(df[column], df[target_variable], normalize='index')
        ctab.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='Paired')
        plt.title(f'Stacked Proportion of {target_variable} by {column}')
        plt.ylabel('Proportion')
        plt.xlabel(column)
        plt.legend(title=target_variable, loc='upper right')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.show()

"""
3. Pre-Processing
"""

# Drop features with no difference across churn status (based on visual inspection)
columns_to_remove = ['gender', 'PhoneService', 'MultipleLines']
df_slim = df.drop(columns=columns_to_remove)

# Identify variable types
numerical_variables = df_slim.select_dtypes(include=['number']).columns.tolist()
categorical_variables = df_slim.select_dtypes(include=['object', 'category']).columns.tolist()

# Remove ID and target from lists
categorical_variables.remove('customerID')
categorical_variables.remove(target_variable)

print("Numerical variables:", numerical_variables)
print("Categorical variables:", categorical_variables)

# One-hot encode categorical features
df_encoded = pd.get_dummies(df_slim, columns=categorical_variables, drop_first=True)

# Convert churn target to binary
df_encoded[target_variable] = df_encoded[target_variable].map({'No': 0, 'Yes': 1})

# Display new DataFrame structure
df_encoded.info()

# Prepare training/validation/test splits
X = df_encoded.drop([target_variable, 'customerID'], axis=1)
y = df_encoded[target_variable]

X_train, X_remaining, y_train, y_remaining = train_test_split(
    X, y, train_size=0.7, random_state=42, stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_remaining, y_remaining, test_size=0.5, random_state=42, stratify=y_remaining
)

print("Training set size:", len(X_train))
print("Validation set size:", len(X_val))
print("Test set size:", len(X_test))

# Scale numerical features
scaler = StandardScaler()

X_train_scaled = X_train.copy()
X_val_scaled = X_val.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numerical_variables] = scaler.fit_transform(X_train[numerical_variables])
X_val_scaled[numerical_variables] = scaler.transform(X_val[numerical_variables])
X_test_scaled[numerical_variables] = scaler.transform(X_test[numerical_variables])

# Sanity check scaled data
print("Scaled Training Data (first 5 rows):")
print(X_train_scaled.head())

print("\nSummary Statistics of Scaled Training Data:")
print(X_train_scaled[numerical_variables].describe())

print("\nSummary Statistics of Scaled Validation Data:")
print(X_val_scaled[numerical_variables].describe())

print("\nSummary Statistics of Scaled Test Data:")
print(X_test_scaled[numerical_variables].describe())

"""
4. Logistic Regression Model as a Baseline
"""
"""
4. a) Model Training
"""

# Train a logistic regression model
lr = LogisticRegression(class_weight='balanced', max_iter=1000)
lr.fit(X_train_scaled, y_train)

# Use custom threshold of 0.45 for predictions
y_pred_lr = (lr.predict_proba(X_test_scaled)[:, 1] >= 0.45).astype(int)

print("\n Logistic Regression Performance:")
print(classification_report(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, lr.predict_proba(X_test_scaled)[:, 1]))

# Tune decision threshold using validation set (maximise F1)
y_scores = lr.predict_proba(X_val_scaled)[:, 1]
precision, recall, thresholds = precision_recall_curve(y_val, y_scores)
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)
best_index = np.argmax(f1_scores)
best_threshold = thresholds[best_index]
best_f1 = f1_scores[best_index]

print(f"Optimal threshold: {best_threshold:.2f} with F1 score: {best_f1:.3f}")

# Plot precision-recall curve
y_scores_test = lr.predict_proba(X_test_scaled)[:, 1]
precision, recall, _ = precision_recall_curve(y_test, y_scores_test)
avg_precision = average_precision_score(y_test, y_scores_test)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'Precision-Recall curve (AP = {avg_precision:.2f})', color='teal')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Churn Prediction')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""
4. b) Model Evaluation
"""

# Plot ROC curve
fpr, tpr, _ = roc_curve(y_val, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

"""
4. c) Model Interpretation
"""

# Visualise logistic regression coefficients
coefficients = lr.coef_[0]
features = X_train.columns

coef_df = pd.DataFrame({
    'Feature': features,
    'Coefficient': coefficients
})
coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()
coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(
    data=coef_df.head(15),
    x='Coefficient',
    y='Feature',
    palette='coolwarm'
)
plt.axvline(0, color='gray', linestyle='--')
plt.title('Top Factors Driving Churn (Logistic Regression Coefficients)')
plt.xlabel('Coefficient Value (positive = higher churn probability)')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""
5. XGBoost Model to Improve Accuracy
"""

"""
5. a) Model Training
"""

# Convert boolean variables to float
X_train_scaled = X_train_scaled.astype({
    col: 'float64' for col in X_train_scaled.select_dtypes('bool').columns
})
X_test_scaled = X_test_scaled.astype({
    col: 'float64' for col in X_test_scaled.select_dtypes('bool').columns
})

# Define parameter grid for RandomizedSearchCV
param_grid = {
    'n_estimators': [100, 300, 500, 800],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'min_child_weight': [1, 3, 5, 10],
    'gamma': [0, 0.1, 0.3, 0.5],
    'scale_pos_weight': [
        (y_train == 0).sum() / (y_train == 1).sum(), 1.5, 2
    ]
}

xgb_base = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='aucpr',
    use_label_encoder=False,
    random_state=42,
    verbosity=0
)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

random_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_grid,
    n_iter=50,
    scoring='roc_auc',
    n_jobs=-1,
    cv=cv,
    verbose=1,
    random_state=42
)

random_search.fit(X_train_scaled, y_train)

best_params = random_search.best_params_

xgb_final = xgb.XGBClassifier(
    **best_params,
    objective='binary:logistic',
    eval_metric='aucpr',
    use_label_encoder=False,
    random_state=42,
    verbosity=0
)

xgb_final.fit(X_train_scaled, y_train)

# Predict and evaluate
y_proba_xgb = xgb_final.predict_proba(X_test_scaled)[:, 1]
threshold = 0.25
y_pred_xgb = (y_proba_xgb >= threshold).astype(int)

print("\n Best Parameters Found:")
print(best_params)

print("\n XGBoost Performance (with threshold 0.25):")
print(classification_report(y_test, y_pred_xgb))
print("ROC-AUC:", roc_auc_score(y_test, y_proba_xgb))

# Find optimal threshold using F1 score
y_scores = xgb_final.predict_proba(X_val_scaled)[:, 1]
precision, recall, thresholds = precision_recall_curve(y_val, y_scores)
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)
best_index = np.argmax(f1_scores)
best_threshold = thresholds[best_index]
best_f1 = f1_scores[best_index]

print(f"Optimal threshold: {best_threshold:.2f} with F1 score: {best_f1:.3f}")

# Precision-recall curve
y_scores = xgb_final.predict_proba(X_test_scaled)[:, 1]
precision, recall, thresholds = precision_recall_curve(y_test, y_scores)
avg_precision = average_precision_score(y_test, y_scores)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='teal')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

"""
5. b) Model Evaluation
"""

# ROC curve
y_scores = xgb_final.predict_proba(X_val_scaled)[:, 1]
fpr, tpr, thresholds = roc_curve(y_val, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""
5. c) Model Interpretation
"""

# SHAP analysis
explainer = shap.Explainer(xgb_final, X_train_scaled)
shap_values = explainer(X_test_scaled)

shap.summary_plot(shap_values, X_test_scaled, plot_type="bar")
shap.summary_plot(shap_values, X_test_scaled)
shap.plots.waterfall(shap_values[0])

# Score full dataset
X_all_scaled = df_encoded.copy()
X_all_scaled[numerical_variables] = scaler.transform(X[numerical_variables])
X_all_scaled = X_all_scaled.astype({
    col: 'float64' for col in X_all_scaled.select_dtypes('bool').columns
})

df_scored = df.copy()
df_scored['propensity_to_churn'] = xgb_final.predict_proba(
    X_all_scaled.drop(['customerID', 'Churn'], axis=1)
)[:, 1]
df_scored['predicted_churn_xgb'] = (
    df_scored['propensity_to_churn'] >= threshold).astype(int)

print("\nPredicted Churn Output:")
print(df_scored[['customerID', 'Churn', 'propensity_to_churn',
                 'predicted_churn_xgb']].head(10))

# Feature analysis
variables_to_plot = [
    'tenure', 'Contract', 'InternetService',
    'MonthlyCharges', 'PaymentMethod', 'PaperlessBilling'
]

numerical_vars_to_plot = [
    var for var in variables_to_plot
    if df_scored[var].dtype != 'object' and df_scored[var].nunique() >= 10
]
categorical_vars_to_plot = [
    var for var in variables_to_plot
    if df_scored[var].dtype == 'object' or df_scored[var].nunique() < 10
]

# Numerical variable plots
for var in numerical_vars_to_plot:
    plt.figure(figsize=(10, 6))
    sns.regplot(data=df_scored, x=var, y='propensity_to_churn',
                scatter_kws={'alpha': 0.3})
    plt.title(f'Churn vs. {var}')
    plt.grid(True)
    plt.show()

# Categorical variable plots
for var in categorical_vars_to_plot:
    ctab = pd.crosstab(
        df_scored[var], df_scored['predicted_churn_xgb'],
        normalize='index'
    )
    ctab.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')
    plt.title(f'Predicted Churn by {var}')
    plt.ylabel('Proportion')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Confusion matrix
cm = confusion_matrix(
    df_scored['Churn'].map({'No': 0, 'Yes': 1}),
    df_scored['predicted_churn_xgb']
)
cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

plt.figure(figsize=(8, 6))
sns.heatmap(cm_percent, annot=True, fmt=".1f", cmap="Blues", cbar=True,
            xticklabels=['Predicted No Churn', 'Predicted Churn'],
            yticklabels=['Actual No Churn', 'Actual Churn'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix (%)')
plt.show()

print("\nConfusion Matrix:")
print(cm)
print("\nConfusion Matrix (Percentages):")
print(cm_percent)

"""
Part 2 - Generating Retention Emails
"""

"""
1. Prepare Prompt Input
"""

# Prepare and score all customers using the trained model

# Scale numerical variables
X_all_scaled = df_encoded.copy()
X_all_scaled[numerical_variables] = scaler.transform(
    X_all_scaled[numerical_variables]
)

# Convert boolean columns to float64 for XGBoost
X_all_scaled = X_all_scaled.astype({
    col: 'float64' for col in X_all_scaled.select_dtypes('bool').columns
})

# Predict churn probabilities
df_scored = df.copy()
df_scored['propensity_to_churn'] = xgb_final.predict_proba(
    X_all_scaled.drop(['customerID', 'Churn'], axis=1)
)[:, 1]

# Classify customers based on threshold
df_scored['predicted_churn_xgb'] = (
    df_scored['propensity_to_churn'] >= threshold).astype(int)

# Check output
print(df_scored[[
    'customerID', 'Churn', 'propensity_to_churn', 'predicted_churn_xgb'
]].head(10))

# Select 2 customers predicted to churn
churn_customers = df_scored[df_scored['predicted_churn_xgb'] == 1]
example_customers_df = churn_customers.sample(n=2, random_state=42)
example_customer_ids = example_customers_df['customerID'].tolist()

# Extract fields for prompt input
example_prompt_input = df_scored[df_scored['customerID'].isin(example_customer_ids)][[
    'customerID', 'tenure', 'MonthlyCharges', 'StreamingTV',
    'StreamingMovies', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'propensity_to_churn'
]].rename(columns={
    'customerID': 'Name',
    'tenure': 'Tenure',
    'propensity_to_churn': 'PredictedChurnProbability'
}).reset_index(drop=True)

# Reindex X_all_scaled to customerID
X_all_scaled_indexed = X_all_scaled.set_index('customerID')
X_features_only = X_all_scaled_indexed.drop(columns=['Churn'], errors='ignore')

# Get model-ready input for selected customers
X_examples_model_input = X_features_only.loc[example_customer_ids]

# Generate SHAP values for selected customers
explainer = shap.Explainer(xgb_final)
shap_values_example = explainer(X_examples_model_input)


"""
2. Generate Prompt
"""


def adjust_urgency(churn_prob):
    """
    Returns an appropriate message to guide email tone based on a customer's
    predicted churn probability.

    Parameters:
        churn_prob (float): The probability that the customer will churn.

    Returns:
        str: Guidance text describing how to frame the retention message.
    """
    if churn_prob > 0.8:
        return (
            "This customer is at very high risk of churning. Offer an enticing "
            "exclusive deal in the subject line, and highlight service improvements "
            "and benefits."
        )
    elif churn_prob > 0.6:
        return (
            "Customer shows moderate signs of dissatisfaction. Highlight benefits "
            "and service improvements, and consider offering an exclusive deal."
        )
    else:
        return (
            "Customer shows a mild churn risk. Focus on appreciation and value "
            "reinforcement."
        )


def get_top_churn_drivers(shap_values_row, customer_id, df_original, top_n=3):
    """
    Identifies the top N features driving a customer's churn risk using SHAP values.

    Parameters:
        shap_values_row (shap._explanation.Explanation): SHAP explanation for one customer.
        customer_id (str): The customer's unique ID.
        df_original (pd.DataFrame): The original (unencoded) customer data.
        top_n (int): Number of top features to return (default is 3).

    Returns:
        str: A formatted string listing the top churn drivers with original feature values and SHAP impact.
    """
    shap_values = shap_values_row.values
    feature_names = shap_values_row.feature_names
    shap_dict = dict(zip(feature_names, shap_values))

    # Sort features by absolute SHAP impact
    sorted_features = sorted(
        shap_dict.items(), key=lambda x: abs(x[1]), reverse=True
    )

    # Get original row
    original_row = df_original[df_original['customerID'] == customer_id].iloc[0]

    top_drivers = []
    added_base_features = set()

    for feature, shap_value in sorted_features:
        if len(top_drivers) >= top_n:
            break

        # Handle one-hot encoding by extracting base feature
        base_feature = feature.split('_')[0] if '_' in feature else feature

        if base_feature in added_base_features:
            continue

        original_value = original_row.get(base_feature, 'N/A')
        top_drivers.append(
            f"- {base_feature}: {original_value} (impact: {shap_value:+.3f})"
        )
        added_base_features.add(base_feature)

    return "\n".join(top_drivers)


def create_prompt(name, tenure, monthlycharges, streamingtv, streamingmovies, onlinesecurity,
                  onlinebackup, deviceprotection, predictedchurnprobability,
                  churn_drivers_summary):
    """
    Creates a prompt string for a language model to generate a personalised
    customer retention email.

    Parameters:
        name (str): Customer's name or ID.
        tenure (int): Number of months the customer has been with Vodafone.
        monthlycharges (float): Monthly billing amount in GBP.
        streamingtv (str): Subscription to TV streaming (yes/no).
        streamingmovies (str): Subscription to movie streaming (yes/no).
        onlinesecurity (str): Subscription to online security (yes/no).
        onlinebackup (str): Subscription to online backup (yes/no).
        deviceprotection (str): Subscription to device protection (yes/no).
        predictedchurnprobability (float): Churn probability score from model.
        churn_drivers_summary (str): SHAP-based feature explanation.

    Returns:
        str: A formatted prompt for use with an LLM.
    """
    urgency = adjust_urgency(predictedchurnprobability)

    return f"""
You are a Vodafone customer care representative specialising in customer retention.
Your job is to write short, personalised emails that rebuild trust and show customers
they're valued, while using Vodafone's tone of voice: friendly, clear, helpful,
and professional.

Customer details:
- Name: {name}
- TV Streaming: {streamingtv}
- Movie Streaming: {streamingmovies}
- Monthly Charges: Â£{monthlycharges:.2f}
- Tenure: {tenure} months
- Online Security Subscription: {onlinesecurity}
- Online Backup Subscription: {onlinebackup}
- Device Protection Subscription: {deviceprotection}
- Predicted Churn Probability: {predictedchurnprobability:.2f}

Guidance:
{urgency}

Factors influencing this customer's likelihood to leave include:
{churn_drivers_summary}
Please tailor your retention strategy based on these drivers.

Avoid:
- Jargon
- Generic or impersonal messages
- Referencing the fact that their account has been flagged as a flight risk
- Telling the customer they can cancel at any time or pointing them towards where
  to manage their account

Structure the email as follows:
1. Subject Line: Friendly and enticing, personalised if possible.
2. Greeting: Warm and personal. Address the customer by their first name.
3. Introduction: Acknowledge loyalty and explain why you're reaching out.
4. Body: Include 2â€“3 bullet points of benefits or recent improvements.
5. Call to Action: Friendly and actionable.
6. Closing: Warm and appreciative.
7. Signature: End with "Vodafone Customer Care Team"

Create a clean, professional, visual design for the email including Vodafone's branding.
Output only the email text, no commentary.
"""


"""
3. Generate Email
"""

# Authenticate with Gemini API
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

# Load Gemini model (use Gemini Pro for general-purpose text tasks)
model = genai.GenerativeModel("gemini-1.5-flash")


def generate_email(prompt):
    """
    Sends a prompt to the Gemini model and returns the generated email text.

    Parameters:
        prompt (str): The text prompt containing customer info and instructions.

    Returns:
        str: The generated email content.
    """
    response = model.generate_content(prompt)
    return response.text


for i, row in example_prompt_input.iterrows():
    customer_id = row['Name']
    shap_row = shap_values_example[i]

    top_drivers = get_top_churn_drivers(
        shap_row,
        customer_id,
        df_original=df,
        top_n=3
    )

    prompt = create_prompt(
        name=row['Name'],
        tenure=row['Tenure'],
        monthlycharges=row['MonthlyCharges'],
        streamingtv=row['StreamingTV'],
        streamingmovies=row['StreamingMovies'],
        onlinesecurity=row['OnlineSecurity'],
        onlinebackup=row['OnlineBackup'],
        deviceprotection=row['DeviceProtection'],
        predictedchurnprobability=row['PredictedChurnProbability'],
        churn_drivers_summary=top_drivers
    )

    email = generate_email(prompt)
    print("=" * 80)
    print(
        f"ðŸ“§ Email for {row['Name']} "
        f"(Churn Probability: {row['PredictedChurnProbability']:.2f})"
    )
    print(email)